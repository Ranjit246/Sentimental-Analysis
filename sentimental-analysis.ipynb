{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:49:20.598283Z","iopub.execute_input":"2022-07-27T08:49:20.598908Z","iopub.status.idle":"2022-07-27T08:49:31.450901Z","shell.execute_reply.started":"2022-07-27T08:49:20.598810Z","shell.execute_reply":"2022-07-27T08:49:31.449813Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Cloning Transformer","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers.git","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:49:52.918159Z","iopub.execute_input":"2022-07-27T08:49:52.918908Z","iopub.status.idle":"2022-07-27T08:50:04.810939Z","shell.execute_reply.started":"2022-07-27T08:49:52.918867Z","shell.execute_reply":"2022-07-27T08:50:04.809809Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'transformers'...\nremote: Enumerating objects: 101468, done.\u001b[K\nremote: Counting objects: 100% (167/167), done.\u001b[K\nremote: Compressing objects: 100% (75/75), done.\u001b[K\nremote: Total 101468 (delta 94), reused 122 (delta 76), pack-reused 101301\u001b[K\nReceiving objects: 100% (101468/101468), 94.93 MiB | 28.20 MiB/s, done.\nResolving deltas: 100% (74855/74855), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ./transformers","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:51:06.977760Z","iopub.execute_input":"2022-07-27T08:51:06.978123Z","iopub.status.idle":"2022-07-27T08:51:06.985336Z","shell.execute_reply.started":"2022-07-27T08:51:06.978094Z","shell.execute_reply":"2022-07-27T08:51:06.984066Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install .","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:51:09.927312Z","iopub.execute_input":"2022-07-27T08:51:09.928051Z","iopub.status.idle":"2022-07-27T08:51:42.664219Z","shell.execute_reply.started":"2022-07-27T08:51:09.928011Z","shell.execute_reply":"2022-07-27T08:51:42.663111Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Processing /kaggle/working/transformers\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (0.8.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (4.12.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.21.0.dev0) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.21.0.dev0) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.21.0.dev0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.21.0.dev0) (2022.6.15)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.21.0.dev0) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.21.0.dev0) (1.26.9)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.21.0.dev0-py3-none-any.whl size=4655272 sha256=295adae59ee999f23f4c888490c32d5f24ece74699578f622d07ce9a42ed9ec5\n  Stored in directory: /tmp/pip-ephem-wheel-cache-4v97jrs8/wheels/be/1e/28/7186a3baa6fcb4e9201f390b70b4e6d75651e85d4e8a9ae413\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\nallennlp 2.10.0 requires transformers<4.21,>=4.1, but you have transformers 4.21.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed transformers-4.21.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# loading the imdb dataset\n(ds_train,ds_test), ds_info = tfds.load('imdb_reviews',\n    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n    as_supervised = True,\n    with_info = True      \n)\n\nprint('info', ds_info)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:53:52.219307Z","iopub.execute_input":"2022-07-27T08:53:52.219853Z","iopub.status.idle":"2022-07-27T08:55:02.599763Z","shell.execute_reply.started":"2022-07-27T08:53:52.219809Z","shell.execute_reply":"2022-07-27T08:55:02.598756Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2022-07-27 08:54:02.040203: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d414fd8f730e416eb08faa41eafb9bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738e6ec4f90c4d4d917c4d0ef5d3200c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"2022-07-27 08:54:56.881727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:54:56.882831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:54:56.883519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:54:56.885866: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-27 08:54:56.886188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:54:56.886888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:54:56.887556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:55:02.180138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:55:02.181031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:55:02.181691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-27 08:55:02.182274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"info tfds.core.DatasetInfo(\n    name='imdb_reviews',\n    full_name='imdb_reviews/plain_text/1.0.0',\n    description=\"\"\"\n    Large Movie Review Dataset.\n    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n    \"\"\",\n    config_description=\"\"\"\n    Plain text\n    \"\"\",\n    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n    data_path='/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n    download_size=80.23 MiB,\n    dataset_size=129.83 MiB,\n    features=FeaturesDict({\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n        'text': Text(shape=(), dtype=tf.string),\n    }),\n    supervised_keys=('text', 'label'),\n    disable_shuffling=False,\n    splits={\n        'test': <SplitInfo num_examples=25000, num_shards=1>,\n        'train': <SplitInfo num_examples=25000, num_shards=1>,\n        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n    },\n    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n      title     = {Learning Word Vectors for Sentiment Analysis},\n      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n      month     = {June},\n      year      = {2011},\n      address   = {Portland, Oregon, USA},\n      publisher = {Association for Computational Linguistics},\n      pages     = {142--150},\n      url       = {http://www.aclweb.org/anthology/P11-1015}\n    }\"\"\",\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Viewing the dataset\n\nprint(tfds.as_numpy(ds_train.take(1)))\n\nfor review, label in tfds.as_numpy(ds_train.take(5)):\n    print('review', review.decode()[0:50], label)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:55:39.282861Z","iopub.execute_input":"2022-07-27T08:55:39.283251Z","iopub.status.idle":"2022-07-27T08:55:39.390975Z","shell.execute_reply.started":"2022-07-27T08:55:39.283220Z","shell.execute_reply":"2022-07-27T08:55:39.389928Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<tensorflow_datasets.core.dataset_utils._IterableDataset object at 0x7f3468d67d50>\nreview This was an absolutely terrible movie. Don't be lu 0\nreview I have been known to fall asleep during films, but 0\nreview Mann photographs the Alberta Rocky Mountains in a  0\nreview This is the kind of film for a snowy Sunday aftern 1\nreview As others have mentioned, all the women that go nu 1\n","output_type":"stream"},{"name":"stderr","text":"2022-07-27 08:55:39.330483: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-07-27 08:55:39.381044: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertForSequenceClassification\n\n# model instantiation\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n\n# tokenizer instantiation\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:56:05.757927Z","iopub.execute_input":"2022-07-27T08:56:05.758326Z","iopub.status.idle":"2022-07-27T08:56:23.417624Z","shell.execute_reply.started":"2022-07-27T08:56:05.758294Z","shell.execute_reply":"2022-07-27T08:56:23.416650Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8480aa8dc74c9f972a20313fd7e12a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d92da75514547da9ae417a6a68bb1d1"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127b4c3a3afb42cdbdc5ea524f57d80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c627a39e10e4daeb704879c88a56b5d"}},"metadata":{}}]},{"cell_type":"code","source":"# max_length of the input sentence\nmax_length = 512\n\n# batch size of the input\nbatch_size = 6    # 4 --> 6","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:57:02.192019Z","iopub.execute_input":"2022-07-27T08:57:02.192958Z","iopub.status.idle":"2022-07-27T08:57:02.200088Z","shell.execute_reply.started":"2022-07-27T08:57:02.192914Z","shell.execute_reply":"2022-07-27T08:57:02.198624Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Converting each encoded input into a dictionary\ndef map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n\n  return {\n\n    \"input_ids\":input_ids,\n    \"token_type_ids\": token_type_ids,\n    \"attention_mask\": attention_masks  \n  }, label\n\n\n# Encoding all the examples\ndef encode_examples(ds, limit=-1):\n\n  # Storing all the encoded inputs in lists\n  input_ids_list = []\n  token_type_ids_list = []\n  attention_mask_list = []\n  label_list = []\n\n  if (limit > 0):\n    ds = ds.take(limit)\n\n\n  for review, label in tfds.as_numpy(ds):\n\n    # Converting inputs into features\n    bert_input = tokenizer.encode_plus(review.decode(),\n                               add_special_tokens=True,\n                               max_length=max_length,\n                               pad_to_max_length = True,\n                               truncation=True,\n                               return_attention_mask=True) \n\n    input_ids_list.append(bert_input[\"input_ids\"])\n    token_type_ids_list.append(bert_input[\"token_type_ids\"])\n    attention_mask_list.append(bert_input[\"attention_mask\"])\n    label_list.append([label])\n\n  # Returning the dataset as a properly shaped tensor\n  print(tf.data.Dataset.from_tensor_slices((input_ids_list,token_type_ids_list,attention_mask_list, label_list)).map(map_example_to_dict))\n  return tf.data.Dataset.from_tensor_slices((input_ids_list,token_type_ids_list,attention_mask_list, label_list)).map(map_example_to_dict)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:57:26.077490Z","iopub.execute_input":"2022-07-27T08:57:26.077836Z","iopub.status.idle":"2022-07-27T08:57:26.089191Z","shell.execute_reply.started":"2022-07-27T08:57:26.077808Z","shell.execute_reply":"2022-07-27T08:57:26.088229Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Encoding the train dataset\nds_train_encoded = encode_examples(ds_train).shuffle(1000).batch(batch_size)   \n# Encoding the test dataset\nds_test_encoded = encode_examples(ds_test).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T08:57:42.573007Z","iopub.execute_input":"2022-07-27T08:57:42.573387Z","iopub.status.idle":"2022-07-27T09:13:23.546649Z","shell.execute_reply.started":"2022-07-27T08:57:42.573354Z","shell.execute_reply":"2022-07-27T09:13:23.545632Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"<MapDataset shapes: ({input_ids: (512,), token_type_ids: (512,), attention_mask: (512,)}, (1,)), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n<MapDataset shapes: ({input_ids: (512,), token_type_ids: (512,), attention_mask: (512,)}, (1,)), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine tuning the optimizer \noptimizer = tf.keras.optimizers.Adam(learning_rate=2.5e-5, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:13:23.593018Z","iopub.execute_input":"2022-07-27T09:13:23.593350Z","iopub.status.idle":"2022-07-27T09:13:23.608652Z","shell.execute_reply.started":"2022-07-27T09:13:23.593321Z","shell.execute_reply":"2022-07-27T09:13:23.607616Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Training the model\nhistory = model.fit(ds_train_encoded, epochs = 3, validation_data=ds_test_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:13:23.609955Z","iopub.execute_input":"2022-07-27T09:13:23.610276Z","iopub.status.idle":"2022-07-27T11:23:22.961087Z","shell.execute_reply.started":"2022-07-27T09:13:23.610240Z","shell.execute_reply":"2022-07-27T11:23:22.960028Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/3\n4167/4167 [==============================] - 2595s 617ms/step - loss: 0.2785 - accuracy: 0.8794 - val_loss: 0.2031 - val_accuracy: 0.9218\nEpoch 2/3\n4167/4167 [==============================] - 2563s 615ms/step - loss: 0.1490 - accuracy: 0.9496 - val_loss: 0.3144 - val_accuracy: 0.9164\nEpoch 3/3\n4167/4167 [==============================] - 2562s 615ms/step - loss: 0.0866 - accuracy: 0.9744 - val_loss: 0.4657 - val_accuracy: 0.9224\n","output_type":"stream"}]}]}